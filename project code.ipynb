{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "import keras \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Dense, Activation, Dropout, Flatten, AveragePooling2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras import regularizers, initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter to resize the image\n",
    "IMG_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data file\n",
    "\n",
    "LABEL_LIST = pd.read_csv('/home/aleksandra/Desktop/ML project/list_patition_label.txt', \n",
    "                         names=('Iname','Class'), sep=\" \")\n",
    "\n",
    "# Separate two datasets label\n",
    "TRAIN_LIST = LABEL_LIST[LABEL_LIST['Iname'].str.contains(\"train\")]\n",
    "TEST_LIST = LABEL_LIST[LABEL_LIST['Iname'].str.contains(\"test\")]\n",
    "TEST_LIST = TEST_LIST.reset_index()\n",
    "TEST_LIST = TEST_LIST.drop(columns='index')\n",
    "\n",
    "# The path to the folder with images\n",
    "folder_img = '/home/aleksandra/Desktop/ML project/aligned'\n",
    "\n",
    "# Data set creation function\n",
    "def create_data_set(path_file,list_label):\n",
    "    data_set =[]\n",
    "    for img in tqdm(os.listdir(path_file)):\n",
    "        path = os.path.join(path_file,img)\n",
    "        for i in range(len(list_label.index)):\n",
    "            if img.split('_aligned.jpg')[0] == list_label.loc[i,'Iname'].split('.jpg')[0]:\n",
    "                limg = cv2.imread(path, cv2.IMREAD_GRAYSCALE)   \n",
    "                limg = cv2.resize(limg, (IMG_SIZE,IMG_SIZE))\n",
    "                data_set.append([np.array(limg),list_label.loc[i,'Class']])\n",
    "    return data_set\n",
    "\n",
    "# Calling a function and saving the results for the training dataset\n",
    "train_data = create_data_set(folder_img, TRAIN_LIST) \n",
    "shuffle(train_data)\n",
    "np.save('/home/aleksandra/Desktop/ML project/train_data.npy', train_data)\n",
    "\n",
    "# Calling a function and saving the results for the test dataset\n",
    "test_data = create_data_set(folder_img, TEST_LIST)\n",
    "shuffle(test_data)\n",
    "np.save('/home/aleksandra/Desktop/ML project/test_data.npy', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload dataset\n",
    "train_data = np.load('/home/aleksandra/Desktop/ML project/train_data.npy',allow_pickle=True)\n",
    "test_data = np.load('/home/aleksandra/Desktop/ML project/test_data.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data for data training and validation\n",
    "train = train_data[:-2500]\n",
    "test = train_data[-2500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resize and normalize data training and data validation\n",
    "train_x = np.array([i[0]/255 for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "train_y = np.array([i[1]-1 for i in train])\n",
    "train_y = np_utils.to_categorical(train_y)\n",
    "\n",
    "test_x = np.array([i[0]/255 for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = np.array([i[1]-1 for i in test])\n",
    "test_y = np_utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Lenet-5 model \n",
    "def Lenet5_classic(train_x, train_y,test_x,test_y):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(units=120, activation='relu'))\n",
    "    model1.add(Dense(units=84, activation='relu'))\n",
    "    model1.add(Dense(units=7, activation = 'softmax'))\n",
    "    model1.build()\n",
    "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history = model1.fit(train_x, train_y, batch_size=128, epochs=10,validation_data=(test_x,test_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1)])\n",
    "    model1.save(\"lenet5_classic.h5\")\n",
    "    np.save('history_classic.npy',history.history)\n",
    "    return(model1,history)\n",
    "\n",
    "# Call a function\n",
    "model_c, history_c = Lenet5_classic(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "prediction = model_c.predict(test_X)\n",
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet-5 model with dropout\n",
    "def Lenet5_dropout(train_x, train_y,test_x,test_y):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(units=120, activation='relu'))\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=84, activation='relu'))\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=7, activation = 'softmax'))\n",
    "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history = model1.fit(train_x, train_y, batch_size=128, epochs=40,validation_data=(test_x,test_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1)])\n",
    "    model1.save(\"lenet5_dropout.h5\")\n",
    "    np.save('history_dropout.npy',history.history)\n",
    "    return(model1,history)\n",
    "\n",
    "# Call a function\n",
    "model_d, history_d = Lenet5_dropout(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "prediction = model_d.predict(test_X)\n",
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet-5 model with adding two additional convolutional layers\n",
    "def Lenet5_model1(train_x, train_y,test_x,test_y):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(Conv2D(filters=6, kernel_size=(3,3), input_shape=(32,32,1), activation='relu')\n",
    "    model1.add(Conv2D(filters=12, kernel_size=(4,4), activation='relu')\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(units=120, activation='relu'\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=84, activation='relu')\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=7, activation = 'softmax')\n",
    "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history = model1.fit(train_x, train_y, batch_size=128, epochs=40,validation_data=(test_x,test_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1)])\n",
    "    model1.save(\"lenet5_model1.h5\")\n",
    "    np.save('history_model1.npy',history.history)\n",
    "    return(model1,history)\n",
    "\n",
    "# Call a function\n",
    "model_1, history_1 = Lenet5_model1(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "prediction = model_1.predict(test_X)\n",
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet-5 model with using 3x3 filters on all convolutional layers\n",
    "def Lenet5_model2(train_x, train_y,test_x,test_y):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(Conv2D(filters=6, kernel_size=(3,3), input_shape=(32,32,1), activation='relu')\n",
    "    model1.add(Conv2D(filters=12, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(units=120, activation='relu'\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=84, activation='relu')\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=7, activation = 'softmax')\n",
    "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history = model1.fit(train_x, train_y, batch_size=128, epochs=40,validation_data=(test_x,test_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1)])\n",
    "    model1.save(\"lenet5_model2.h5\")\n",
    "    np.save('history_model2.npy',history.history)\n",
    "    return(model1,history)\n",
    "              \n",
    "# Call a function\n",
    "model_2, history_2 = Lenet5_model2(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "prediction = model_2.predict(test_X)\n",
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet-5 model with BatchNormalization\n",
    "def Lenet5_model3(train_x, train_y,test_x,test_y):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Conv2D(filters=6, kernel_size=(3,3), input_shape=(32,32,1), activation='relu')\n",
    "    model1.add(Conv2D(filters=12, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu')\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(BatchNormalization(axis=1))           \n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(units=120, activation='relu'\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=84, activation='relu')\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=7, activation = 'softmax')\n",
    "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history = model1.fit(train_x, train_y, batch_size=128, epochs=40,validation_data=(test_x,test_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1)])\n",
    "    model1.save(\"lenet5_model3.h5\")\n",
    "    np.save('history_model3.npy',history.history)\n",
    "    return(model1,history)\n",
    "  \n",
    "# Call a function\n",
    "model_3, history_3 = Lenet5_model3(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "prediction = model_3.predict(test_X)\n",
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lenet-5 model with application of He initializers, Xavier initializer,l2-regularizer\n",
    "def Lenet5_model4(train_x, train_y,test_x,test_y):\n",
    "    model1 = keras.Sequential()\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Conv2D(filters=6, kernel_size=(3,3), input_shape=(32,32,1), activation='relu',kernel_initializer=initializers.HeUniform(), kernel_regularizer=regularizers.L2(1e-4),bias_initializer=initializers.HeUniform(),bias_regularizer=regularizers.L2(1e-4)))\n",
    "    model1.add(Conv2D(filters=12, kernel_size=(3,3), activation='relu',kernel_initializer=initializers.HeUniform(), kernel_regularizer=regularizers.L2(1e-4),bias_initializer=initializers.HeUniform(),bias_regularizer=regularizers.L2(1e-4)))\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', kernel_initializer=initializers.HeUniform(),bias_initializer=initializers.HeUniform(),bias_regularizer=regularizers.L2(1e-4), kernel_regularizer=regularizers.L2(1e-4)))\n",
    "    model1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', kernel_initializer=initializers.HeUniform(),bias_initializer=initializers.HeUniform(),bias_regularizer=regularizers.L2(1e-4), kernel_regularizer=regularizers.L2(1e-4)))\n",
    "    model1.add(AveragePooling2D(strides=2))\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dense(units=120, activation='relu',bias_initializer=initializers.HeUniform(),bias_regularizer=regularizers.L2(1e-4), kernel_regularizer=regularizers.L2(1e-4), kernel_initializer=initializers.HeUniform()))\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=84, activation='relu',bias_initializer=initializers.HeUniform(),bias_regularizer=regularizers.L2(1e-4), kernel_regularizer=regularizers.L2(1e-4), kernel_initializer=initializers.HeUniform()))\n",
    "    model1.add(BatchNormalization(axis=1))\n",
    "    model1.add(Dropout(0.4))\n",
    "    model1.add(Dense(units=7, activation = 'softmax', kernel_initializer=initializers.GlorotUniform(),kernel_regularizer=regularizers.L2(1e-4),bias_initializer=initializers.GlorotUniform(),bias_regularizer=regularizers.L2(1e-4)))\n",
    "    model1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    history = model1.fit(train_x, train_y, batch_size=128, epochs=40,validation_data=(test_x,test_y),callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=1)])\n",
    "    model1.save(\"lenet5_model4.h5\")\n",
    "    np.save('history_model4.npy',history.history)\n",
    "    return(model1,history)\n",
    "\n",
    "# Call a function\n",
    "model_4, history_4 = Lenet5_model4(train_x, train_y, test_x, test_y)\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "prediction = model_4.predict(test_X)\n",
    "y_pred = np.argmax(prediction, axis=1)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ensemble of model3 and model4\n",
    "\n",
    "# Upload model3 and model4\n",
    "model1 = tf.keras.models.load_model('lenet5_model3.h5')\n",
    "model2 = tf.keras.models.load_model('lenet5_model4.h5')\n",
    "model_list = [model1,model2]\n",
    "\n",
    "def ensemble_prediction(members, testX):\n",
    "    yhats = [model.predict(testX) for model in members]\n",
    "    summed = np.sum(yhats, axis=0)\n",
    "    result = np.argmax(summed, axis=1)\n",
    "    return result\n",
    "\n",
    "# Normalize test data\n",
    "test_X = np.array([i[0]/255 for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = ensemble_prediction(model_list, test_X)\n",
    "y_test = np.array([i[1]-1 for i in test_data])\n",
    "print('Test set error rate: {}'.format(np.mean(y_pred == y_test)))\n",
    "num_classes=7\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
